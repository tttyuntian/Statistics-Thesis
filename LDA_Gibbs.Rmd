---
title: "LDA_Gibbs_Sampling"
author: "Tian (Simon) Yun"
date: "1/24/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, include = FALSE}
library(RSQLite)
library(MCMCpack)
library(dplyr)
library(tidytext)
library(tidyr)
```

```{r}
# Connect to database
db <- dbConnect(dbDriver("SQLite"), "database.sqlite")

# Sampling from the database
papers_sample <- dbGetQuery(db, "
SELECT paper_text
FROM papers
WHERE year > 2015
ORDER BY id DESC
LIMIT 10")
```

```{r}
generate_vocab <- function(W){
  # To generate the vocabulary.
  #
  # Args:
  #   W: The column "word" in W represents the words that have shown up in the papers.
  #
  # Returns:
  #   The vocabulary (list) that includes all the unique words in the papers.
  res = unique(W$word)
  return(res)
}


get_num_words <- function(W, num_document){
  # To get the number of words in each document.
  #
  # Args:
  #   W: The column "documentID" in W represents the ID of each paper.
  #   num_document: The number of documents.
  # 
  # Returns:
  #   A list of number of words in each paper.
  res = rep(0, num_document)
  for (i in 1:num_document){
    res[i] = table(W$documentID)[i]
  }
  return(res)
}


get_customize_stopwords <- function(stop.words = list()){
  # To get the customized stop words list.
  #
  # Args:
  #   stop.words: The to-be-appended stop words list.
  # 
  # Returns:
  #   A list of stop words.
  res = NULL
  # Load default stop words
  data("stop_words")
  # Append new stop words
  res <- bind_rows(tibble(word = stop.words,
                          lexicon = c("custom")),
                   stop_words)
  return(res)
}


get_tokenized_data <- function(corpus, num_document, stop.words = list()){
  # To get the tokenized data with columns "documentID" and "word", and to remove stop words.
  #
  # Args:
  #   corpus: The corpus that includes the papers.
  #   num_document: The number of documents.
  #   stop.words: The to-be-appended stop words list.
  # 
  # Returns:
  #   A tibble with columns "documentID" and "word".
  custom_stop_words = get_customize_stopwords(stop.words)
  # create the tibble with columns "documentID" and "text"
  text_df <- tibble(documentID = 1:num_document, text = corpus[, 1])
  # remove stop words
  res <- text_df %>% 
    unnest_tokens(word, text) %>%
    anti_join(custom_stop_words)
  return(res)
}


init_z <- function(theta, N, num_document){
  # To initialize z-matrix in LDA. This matrix has dimension of (number of documents) * (number of words in each document).
  # (i, j) element in z-matrix represents the topic of j-th word in i-th document.
  #
  # Args:
  #   theta: The theta-matrix in LDA.
  #   N: The number of words in each document.
  #   num_document: The number of documents.
  # 
  # Returns:
  #   An initialized z-matrix.
  res = c()
  for(i in 1:num_document){
    doc_size = N[i]
    cur_doc <- rep(0, doc_size)
    for(j in 1:doc_size){
      temp = rmultinom(1, 1, theta[i,])
      temp <- which.max(temp)
      cur_doc[j] <- temp
    }
    res[[i]] <- cur_doc
  }
  return(res)
}


init_w <- function(W, N, vocabulary, num_document){
  # To initialize w-matrix in LDA. This matrix has dimension of (number of documents) * (number of words in each document).
  # (i, j) element in w-matrix represents the j-th word in i-th document. Each element is an index of the word in vocabulary.
  #
  # Args:
  #   W: The tibble with tokenized words.
  #   N: The number of words in each document.
  #   vocabulary: The vocabulary.
  #   num_document: The number of documents.
  # 
  # Returns:
  #   An initialized w-matrix.
  res = c()
  start = 1
  for(i in 1:num_document){
    doc_size = N[i]
    temp = rep(0, doc_size)
    for(j in 1:doc_size){
      # Find the index of this word in the vocabulary.
      temp[j] = which(vocabulary == W$word[start])
      start = start + 1
    }
    res[[i]] <- temp
  }
  return(res)
}


get_occurrence_of_word_in_topic <- function(z, w, N, num_document, num_topic, num_vocab){
  # To generate a 2D-list list with the number of occurrences of words in each topic.
  #
  # Args:
  #   z: The z-matrix.
  #   w: The w-matrix.
  #   N: The number of words in each document.
  #   num_document: The number of documents.
  #   num_topic: The number of topics.
  #   num_vocab: The size of vocabulary.
  # 
  # Returns:
  #   A 2D-list with the number of occurrences of words in each topic.
  res <- rep(0, num_topic)
  # Initialize empty num_document*num_word list
  for(i in 1:num_topic){
    res[i] <- list(rep(0, num_vocab))
  }
  # Start computing the occurrences of word in topic
  for(i in 1:num_document){ 
    doc_size = N[i]
    for(j in 1:doc_size){
      cur_word = w[[i]][j]
      cur_topic = z[[i]][j]
      res[[cur_topic]][cur_word] = res[[cur_topic]][cur_word] + 1
    }
  }
  return(res)
}


draw_beta <- function(z, w, eta, N, num_document, num_topic, num_vocab){
  # To re-draw the beta-matrix from new computed eta. This matrix has dimension of (number of topics) * (size of vocabulary).
  # (i, j) element in beta-matrix represents the probability of occurrence of j-th word in i-th topic.
  #
  # Args:
  #   z: The z-matrix.
  #   w: The w-matrix.
  #   eta: The hyperparameter eta.
  #   N: The number of words in each document.
  #   num_document: The number of documents.
  #   num_topic: The number of topics.
  #   num_vocab: The size of vocabulary.
  # 
  # Returns:
  #   The re-sampled beta-matrix.
  res <- rep(0, num_topic)
  occur_word_in_topic = get_occurrence_of_word_in_topic(z, w, N, num_document, num_topic, num_vocab)
  for(i in 1:num_topic){
    # Computing new eta for each topic, and then compute new beta for each topic
    new_eta = eta + occur_word_in_topic[[i]]
    res[i] <- list(rdirichlet(1, new_eta)[1,])
  }
  return(res)
}


get_occurrence_of_topic_in_document <- function(z, num_document, num_topic){
  # To generate a 2D-list list with the number of occurrences of topics in each document.
  #
  # Args:
  #   z: The z-matrix.
  #   num_document: The number of documents.
  #   num_topic: The number of topics.
  # 
  # Returns:
  #   A 2D-list list with the number of occurrences of topics in each document.
  res <- rep(0, num_document)
  # Initialize empty num_document*num_topic list
  for(i in 1:num_document){
    res[i] <- list(rep(0, num_topic))
  }
  # Start computing the occurrences of topic in document
  for(i in 1:num_document){
    temp = rep(0, num_topic)
    temp_table = table(z[[i]])
    for(j in 1:num_topic){
      count = temp_table[as.character(j)]
      if(is.na(count)){
        temp[j] = 0
      }
      else{
        temp[j] = count
      }
    }
    res[[i]] <- temp
  }
  return(res)
}


draw_theta <- function(z, alpha, num_document, num_topic){
  # To re-draw the theta-matrix from new computed alpha. This matrix has dimension of (number of documents) * (number of topics).
  # (i, j) element in theta-matrix represents the probability of i-th document being classified as j-th topic.
  #
  # Args:
  #   z: The z-matrix.
  #   alpha: The hyperparameter alpha.
  #   num_document: The number of documents.
  #   num_topic: The number of topics.
  # 
  # Returns:
  #   The re-sampled theta-matrix.
  res <- rep(0, num_document) 
  occur_topic_in_document = get_occurrence_of_topic_in_document(z, num_document, num_topic)
  for(i in 1:num_document){
    # Computing new alpha for each document, and then compute new theta for each document
    new_alpha = alpha + occur_topic_in_document[[i]]
    res[i] <- list(rdirichlet(1, new_alpha)[1,])
  }
  return(res)
}


draw_z <- function(theta, beta, w, N, num_document, num_topic){
  # To re-draw the z-matrix from new computed alpha. This matrix has dimension of (number of documents) * (number of words in each document).
  # (i, j) element in z-matrix represents the topic of j-th word in i-th document.
  #
  # Args:
  #   theta: The theta-matrix.
  #   beta: The beta-matrix.
  #   w: The w-matrix.
  #   N: The number of words in each document.
  #   num_document: The number of documents.
  #   num_topic: The number of topics.
  # 
  # Returns:
  #   The re-sampled z-matrix.
  res <- rep(0, num_document)
  # Initialize empty num_document*num_word list
  for(i in 1:num_document){
    res[i] <- list(rep(0, N[i]))
  }
  # Start computing new z
  for(i in 1:num_document){
    doc_size = N[i]
    for(j in 1:doc_size){
      temp = rep(0, num_topic)
      cur_word = w[[i]][j]
      for(k in 1:num_topic){
        prob_topic = theta[[i]][k]
        prob_word = beta[[k]][cur_word]
        temp[k] <- prob_topic * prob_word
      }
      res[[i]][j] = which.max(rmultinom(1, 1, temp))
    }
  }
  return(res)
}


LDA_Gibbs <- function(corpus, K = 5, control = 1123, burn_in = 200, converge_iteration = 100, threshold = 5e-3, max_iteration = 1000, stop.words = NULL){
  # To implement LDA with Gibbs sampling process. 
  #
  # Args:
  #   corpus: The corpus, which is a data.frame with one column. Each element is a document.
  #   K: The number of latent topics.
  #   control: This is the seed for pseudo-randomness.
  #   burn_in: The number of burn-in iterations before the function starts tracking the values of parameters.
  #   converge_iteration: The number of iterations needed to ensure that the algorithm has converged.
  #   Threshold: The converge threshold for increase/decrease rate of a theta parameter. 
  #   max_iteration: The maximum number of iterations of Gibbs sampling process.
  #   stop.words: The to-be-appended stop words list.
  #   
  # Returns:
  #   res$beta: A tidy table with  three columns, including *topic*, *word*, and *beta*.
  #   res$theta: The final theta-matrix in tidy format.
  #   res$theta_param: A list of theta parameters of a chosen document over iterations. 
  set.seed(control)
  # initialize necessary variables
  print("Start initializing...")
  K = K
  M = dim(corpus)[1]
  W = get_tokenized_data(corpus, M, stop.words)
  N = get_num_words(W, M)
  N_corpus = sum(N)
  # generate vocabulary
  vocabulary = generate_vocab(W)
  V = length(vocabulary)
  # initialize alpha and eta
  alpha = list(rep(1, K))
  alpha = unlist(alpha)
  eta = list(rep(1, V))
  eta = unlist(eta)
  print("Done Initializing.")
  # Start Gibbs sampling process
  # The first iteration
  print("Start burn in process...")
  theta = rdirichlet(M, alpha)
  z = init_z(theta, N, M)
  beta = rdirichlet(K, eta)
  w = init_w(W, N, vocabulary, M)
  # The following iterations based on full conditional distribution
  # Pre-Burn-in for burn_in iterations
  for(i in 1:burn_in){
    beta = draw_beta(z, w, eta, N, M, K, V)
    theta = draw_theta(z, alpha, M, K)
    z = draw_z(theta, beta, w, N, M, K)
  }
  print("Done burn in process.")
  # Extract two parameters from beta and theta
  topic_index = sample(1:K, 1)
  doc_index = sample(1:M, 1)
  print(sprintf("Tracing Document #%d", doc_index))
  theta_param = NULL
  theta_param = rbind(theta_param, theta[[doc_index]])
  theta_param_prev = theta[[doc_index]][topic_index]
  # Initialize the change in parameters, and variables for burn-in plot
  theta_delta = rep(1, K)
  theta_delta_count = rep(0, K)
  # Keep iterating till convergence
  print("Start looking for convergence...")
  iteration_count = 1
  while (iteration_count < max_iteration) { 
    if (any(theta_delta_count == converge_iteration)) {
      print(sprintf("Convergence Found at Iteration %d.", iteration_count))
      break
    }
    iteration_count = iteration_count + 1
    beta = draw_beta(z, w, eta, N, M, K, V)
    theta = draw_theta(z, alpha, M, K)
    z = draw_z(theta, beta, w, N, M, K)
    # Compute new delta's
    theta_param = rbind(theta_param, theta[[doc_index]])
    theta_delta = abs((theta_param[iteration_count,] - theta_param[iteration_count - 1,]) / theta_param[iteration_count - 1,])
    # Update theta_delta_count. First, if any parameter >= threshold, then set to 0. Second, if any parameter < threshold, then plus 1.
    theta_delta_count = theta_delta_count * ((theta_delta < threshold) * 1)
    theta_delta_count = theta_delta_count + ((theta_delta < threshold) * 1)
  }
  print("Done looking for convergence.")
  # Convert beta into tidy format
  beta = data.frame(matrix(unlist(beta), byrow = TRUE, ncol = V))
  colnames(beta) = vocabulary
  beta <- beta %>%
    mutate(topic = row_number()) %>%
    gather(word, beta, -topic)
  # Convert theta into tidy format
  theta = data.frame(matrix(unlist(theta), byrow = TRUE, ncol = K))
  colnames(theta) = as.character(1:K)
  theta <- theta %>%
    mutate(document = row_number()) %>%
    gather(topic, theta, -document)
  # Modify column names of theta_param 
  theta_param = data.frame(theta_param)
  colnames(theta_param) = paste("topic", 1:K, sep = "")
  # Combine beta and theta
  res <- list("beta" = beta, 
              "theta" = theta,
              "theta_param" = theta_param)
  return(res)
}
```

```{r}
# Type your code here
```
